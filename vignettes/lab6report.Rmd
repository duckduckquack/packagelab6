---
title: "lab6report"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{lab6report}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
This vignette is going to represent our lab report for assignment number 6. Also, all the answers to the questions in the lab will be included. 

First, we load the package:
```{r}
library(packagelab6)
```

It is important to point out here that the running time of a function depends on many factors. Below we are going to provide just a general idea of the number of seconds taken from our functions to run as this can vary greatly depending, among others, on the machine the function was run on. All the system.time() and microbenchmark() calls below were run on the same computer and on the same day. However, keep in mind that if you re run the code in the vignette the output can be different. 

## Question 1
### *How much time does it take to run brute_force_knapsack for n = 16 objects?*
The brute force version of the knapsack problem is computationally slow. We need to loop through all the possible combinations of items and pick the one that yields the biggest value given the constraint. Therefore, the for loop has to loop over 2^n elements which implies that the time complexity of the algorithm is O(2^n). 
To get the time taken for the function to run with n = 16 we used two different methods.

Method 1: system.time
```{r, eval = FALSE}
system.time(unoptimized_brute_force_knapsack(knapsack_objects[1:16, ], 2000))
```
| user | system | elapsed |
|-----:|-------:| -------:|  
| 4.34 | 0.000  | 4.35    |


Method 2: microbenchmark 
The microbenchmark function runs the algorithm multiple times to get a more accurate running time. We are going to run the function 10 times.
```{r, eval = FALSE}
microbenchmark::microbenchmark(unoptimized_brute_force_knapsack(knapsack_objects[1:16, ], 2000), times = 10)
```
| min | lq  | mean | median | uq   | max |
|----:|----:|-----:|-------:|----: |----:|
|4.15 |4.18 | 4.26 | 4.24   | 4.33 | 4.44|
Unit: seconds

As you can see, we get a confirmation that our brute force implementation takes around 4 seconds (we look at the median in the case of microbenchmarking) to run with an input size of only 16 objects. 

## Question 2
### *How much time does it take to run knapsack_dynamic with n = 500 objects?*
The dynamic programming version of the knapsack problem performs well in terms of running time. As it contains two nested for loops to fill up the matrix of subproblems, the time complexity is O(n*W) where n is the number of items available and W is the total capacity of the knapsack. 
To get the time taken for the function to run with n = 500 we used two different methods.

Method 1: system.time
```{r, eval = FALSE}
system.time(knapsack_dynamic(knapsack_objects[1:500, ], 2000))
```
| user | system | elapsed |
|-----:|-------:| -------:|  
| 0.717| 0.008  | 0.725   |


Method 2: microbenchmarking 
The microbenchmark function runs the algorithm multiple time to get a more accurate running time. We are going to run the function 10 times.
```{r, eval = FALSE}
microbenchmark::microbenchmark(knapsack_dynamic(knapsack_objects[1:500, ], 2000), times = 10)
```
| min   | lq    | mean    | median | uq     | max    |
|------:|------:|---------|-------:|-------:|-------:|
|631.43 |649.07 | 679.15  | 672.91 | 714.33 | 745.53 |
Unit: milliseconds

As you can see above, using the microbenchmark, the function takes around 0.672 seconds to run which is less than the system.time() output. The microbenchmark output is more reliable as the function is run more than once to get a more accurate running time.

## Question 3
### *How much time does it take to run greedy_knapsack on n = 1000000 objects?*
The greedy version of the knapsack problem is the fastest of all. It has a time complexity of O(n). However, being a greedy heuristic algorithm, the result is not always optimal. However it can be shown that if m is the maximum value of the items that fit into the knapsack, the result of the greedy algorithm is at least m/2. More information can be found [here](https://en.wikipedia.org/wiki/Knapsack_problem).

We tried to run the function on 1 million objects and we waited for an hour. After an hour the function was still running. So the running time is for sure more than an hour. We used only the system.time() function this time around. 
Although the greedy algorithm is so much faster, an input of a million is still too big to be evaluated in a reasonable amount of time from a standard laptop.

## Question 4
### *What performance gain could you get by trying to optimize your code?*
In order to optimize a piece of code, it's important to identify bottlenecks first. This is to avoid focusing on lines of codes that may look time inefficient but in reality they are not. For example, for loops are not always a bottleneck, sometimes their running time is not a problem and it is not worth it to try to optimize them. 
We used the package profvis to identify bottlenecks in our code. 

1) brute_force_knapsack
```{r, eval = FALSE}
profvis::profvis({
  unoptimized_brute_force_knapsack <-
  function(x, W){
    #checks
    #x is a data frame
    stopifnot(is.data.frame(x))
    #W is a number
    stopifnot(is.numeric(W))
    #W is positive
    stopifnot(W >= 0)
    #x has cols v and w
    stopifnot(all(sort(names(x)) == c("v", "w")))
    #v and w are positive
    stopifnot(all(x$v > 0))
    stopifnot(all(x$w > 0))


    #get the number of objects
    n <- length(x$w)

    #get all bit combinations
    m <- 2^n #number of combinations
    comb <- lapply(1:m, function(x){intToBits(x)[1:n]})

    #find total weight and value for each combination
    #total weights for all the combinations
    ws <- sapply(1:m, function(i){sum(x[which(comb[[i]] == 1), ]$w)})
    #total values for all the combinations
    vs <- sapply(1:m, function(i){sum(x[which(comb[[i]] == 1), ]$v)})

    #pick combination with best value given constraint
    max_v <- max(vs[which(ws <= W)]) #max value given constraint

    #get index of combination corresponding to max value given constraint
    vs[which(ws > W)] <- 0
    i_max_v <- which(vs == max_v)[1] #in case of more than one comb yielding max value

    #get objects corresponding to combination
    res <- x[comb[[i_max_v]] == 1, ]

    return(list(value = max_v, elements = as.numeric(row.names(res))))
  }
  unoptimized_brute_force_knapsack(knapsack_objects[1:8, ], 2000)
})
```
From running this function on profvis, we can notice that the two sapplys are the two bottlenecks. We tried to change the two sapplys with vapplys but we did not notice a huge improvement in performance:
```{r, eval = FALSE}
profvis::profvis({
  unoptimized_brute_force_knapsack <-
  function(x, W){
    #checks
    #x is a data frame
    stopifnot(is.data.frame(x))
    #W is a number
    stopifnot(is.numeric(W))
    #W is positive
    stopifnot(W >= 0)
    #x has cols v and w
    stopifnot(all(sort(names(x)) == c("v", "w")))
    #v and w are positive
    stopifnot(all(x$v > 0))
    stopifnot(all(x$w > 0))


    #get the number of objects
    n <- length(x$w)

    #get all bit combinations
    m <- 2^n #number of combinations
    comb <- lapply(1:m, function(x){intToBits(x)[1:n]})

    #find total weight and value for each combination
    #total weights for all the combinations
    ws <- vapply(1:m, function(i){sum(x[which(comb[[i]] == 1), ]$w)}, numeric(1))
    #total values for all the combinations
    vs <- vapply(1:m, function(i){sum(x[which(comb[[i]] == 1), ]$v)}, numeric(1))

    #pick combination with best value given constraint
    max_v <- max(vs[which(ws <= W)]) #max value given constraint

    #get index of combination corresponding to max value given constraint
    vs[which(ws > W)] <- 0
    i_max_v <- which(vs == max_v)[1] #in case of more than one comb yielding max value

    #get objects corresponding to combination
    res <- x[comb[[i_max_v]] == 1, ]

    return(list(value = max_v, elements = as.numeric(row.names(res))))
  }
  unoptimized_brute_force_knapsack(knapsack_objects[1:8, ], 2000)
})
```
However, we can notice that inside vapply the call of the function is taking a large amount of time. So we tried to optimize the function inside the vapply instead. 
```{r, eval = FALSE}
profvis::profvis({
  unoptimized_brute_force_knapsack <-
  function(x, W){
    #checks
    #x is a data frame
    stopifnot(is.data.frame(x))
    #W is a number
    stopifnot(is.numeric(W))
    #W is positive
    stopifnot(W >= 0)
    #x has cols v and w
    stopifnot(all(sort(names(x)) == c("v", "w")))
    #v and w are positive
    stopifnot(all(x$v > 0))
    stopifnot(all(x$w > 0))


    #get the number of objects
    n <- length(x$w)
    
    v <- x$v
    w <- x$w
    
    #get all bit combinations
    m <- 2^n #number of combinations
    comb <- lapply(1:m, function(x){intToBits(x)[1:n]})

    #find total weight and value for each combination
    #total weights for all the combinations
    ws <- vapply(1:m, function(i){sum(w[which(comb[[i]] == 1)])}, numeric(1))
    #total values for all the combinations
    vs <- vapply(1:m, function(i){sum(v[which(comb[[i]] == 1)])}, numeric(1))

    #pick combination with best value given constraint
    max_v <- max(vs[which(ws <= W)]) #max value given constraint

    #get index of combination corresponding to max value given constraint
    vs[which(ws > W)] <- 0
    i_max_v <- which(vs == max_v)[1] #in case of more than one comb yielding max value

    #get objects corresponding to combination
    res <- x[comb[[i_max_v]] == 1, ]

    return(list(value = max_v, elements = as.numeric(row.names(res))))
  }
  unoptimized_brute_force_knapsack(knapsack_objects[1:8, ], 2000)
})
```
From working with a data frame to working with a vector with obtained a good performance improvement. 
Now let's try to run the optimized version of the brute force algorithm with system.time() and microbenchmark:
```{r, eval = FALSE}
system.time(brute_force_knapsack(knapsack_objects[1:16, ], 2000))
```
| user  | system | elapsed |
|------:|-------:| -------:|  
| 0.397 | 0.012  | 0.408   |

```{r, eval = FALSE}
microbenchmark::microbenchmark(brute_force_knapsack(knapsack_objects[1:16, ], 2000), times = 10)
```
| min   | lq    | mean    | median | uq     | max    |
|------:|------:|---------|-------:|-------:|-------:|
|287.80 |315.28 | 324.58  | 327.12 | 340.22 | 352.46 |
Unit: milliseconds

As you can see we greatly improved our running time (compare it with the tables in Question 1).

2) other functions

We also tried the profvis function with the other functions but we didn't identify any bottleneck. Meaning, the only gray rectangle that shows up in the profvis output is on the function call. This is probably due to the fact that the algorithm for those two functions are pretty efficient to begin with. 

## Question 5
### *What performance gain could you get by parallelizing brute force search?*
Initially we tried parallelizing by using platform independent parallelization functions from the parallelize package on the number of detected cores:
```{r, eval = FALSE}
brute_force_knapsack_parallelized <- 
  function(x, W){
    #checks
    #x is a data frame
    stopifnot(is.data.frame(x))
    #W is a number
    stopifnot(is.numeric(W))
    #W is positive
    stopifnot(W >= 0)
    #x has cols v and w
    stopifnot(all(sort(names(x)) == c("v", "w")))
    #v and w are positive
    stopifnot(all(x$v > 0))
    stopifnot(all(x$w > 0))
    
    
    #get the number of objects
    n <- length(x$w) 
    v <- x$v
    w <- x$w
    
    #get all bit combinations
    m <- 2^n #number of combinations
    
    #get the number of cores
    ncores <- parallel::detectCores() 
    cl <- parallel::makeCluster(ncores)
    #find all the combinations in parallel
    comb <- parallel::parLapply(cl, 1:m, function(x){intToBits(x)[1:n]}) 
    #find weights of all the combinations in parallel
    ws <- parallel::parSapply(cl, 1:m, function(i){sum(w[which(comb[[i]] == 1)])})
    #find values of all the combinations in parallel
    vs <- parallel::parSapply(cl, 1:m, function(i){sum(v[which(comb[[i]] == 1)])})
    parallel::stopCluster(cl)
    
    #pick combination with best value given constraint
    max_v <- max(vs[which(ws <= W)]) #max value given constraint
    
    #get index of combination corresponding to max value given constraint
    vs[which(ws > W)] <- 0
    i_max_v <- which(vs == max_v)[1] #in case of more than one comb yielding max value
    
    #get objects corresponding to combination
    res <- x[comb[[i_max_v]] == 1, ]
    
    return(list(value = max_v, elements = as.numeric(row.names(res))))
  }

```

```{r, eval = FALSE}
system.time(brute_force_knapsack_parallelized(knapsack_objects[1:16,], 2000))
```
| user  | system | elapsed |
|------:|-------:| -------:|  
| 0.737 | 0.558  | 0.450   |

As you can see this is a huge improvements from the unoptimized_brute_force_knapsack. However, compared to brute_force_knapsack, this parallelized version performs slightly worse (however this can also be due to other factors not connected with how we wrote our code, the main idea though, is that we don't notice any significant improvment). 
We then decided to try some platform dependent parallelization functions as after doing some research online, we found out that paralellization doesn't always work well on Windows machines. The one below is our final version of the brute force:
```{r, eval = FALSE}
brute_force_knapsack <-
  function(x, W, parallel = FALSE){
    #checks
    #x is a data frame
    stopifnot(is.data.frame(x))
    #W is a number
    stopifnot(is.numeric(W))
    #W is positive
    stopifnot(W >= 0)
    #x has cols v and w
    stopifnot(all(sort(names(x)) == c("v", "w")))
    #v and w are positive
    stopifnot(all(x$v > 0))
    stopifnot(all(x$w > 0))


    #get the number of objects and the vectors v and w
    n <- length(x$w)
    v <- x$v
    w <- x$w

    #get the number of bit combinations
    m <- 2^n #number of combinations

    if(parallel == FALSE){
      #find all the combinations
      comb <- lapply(1:m, function(x){intToBits(x)[1:n]})
      #find total weight and value for each combination
      ws <- vapply(1:m, function(i){sum(w[which(comb[[i]] == 1)])}, numeric(1))
      vs <- vapply(1:m, function(i){sum(v[which(comb[[i]] == 1)])}, numeric(1))
    }else{
      #get the number of cores
      ncores <- parallel::detectCores() 
      #find all the combinations in parallel
      comb <- parallel::mclapply(1:m, function(x){intToBits(x)[1:n]}, mc.cores = ncores)
      #find total weight and value for each combination in parallel
      ws <- as.numeric(parallel::mclapply(1:m, function(i){sum(w[which(comb[[i]] == 1)])}, mc.cores = ncores))
      vs <- as.numeric(parallel::mclapply(1:m, function(i){sum(v[which(comb[[i]] == 1)])}, mc.cores = ncores))
    }


    #pick combination with best value given constraint
    max_v <- max(vs[which(ws <= W)]) #max value given constraint

    #get index of combination corresponding to max value given constraint
    vs[which(ws > W)] <- 0
    i_max_v <- which(vs == max_v)[1] #in case of more than one comb yielding max value

    #get objects corresponding to combination
    res <- x[comb[[i_max_v]] == 1, ]

    return(list(value = max_v, elements = as.numeric(row.names(res))))
  }
```


```{r, eval = FALSE}
system.time(brute_force_knapsack(knapsack_objects[1:16, ], 2000), TRUE)
```
| user  | system | elapsed |
|------:|-------:| -------:|  
| 0.397 | 0.000  | 0.390   |
  
As you can see, we obtain an improvement from the parallelized version that also works on Windows machines. However, given the running time, we can't really say that parallelizing is worth it in this particular situation.
Parallelization takes some resources at the beginning to create the different processes and to initialize the communication between them (overhead time) so it may not be worth it for n = 16. So we tried to time our two versions of the brute force on an input of n = 20.
```{r, eval = FALSE}
system.time(brute_force_knapsack(knapsack_objects[1:20, ], 2000), TRUE)
```
| user  | system | elapsed |
|------:|-------:| -------:|  
| 6.016 | 0.000  | 6.012  |
```{r, eval = FALSE}
system.time(brute_force_knapsack(knapsack_objects[1:20, ], 2000))
```
| user  | system | elapsed |
|------:|-------:| -------:|  
| 6.460 | 0.009  |  6.465  |
As you can see, if we start increasing the input size, it is more clear that the parallelized version performs better. 
In conclusion, it is better to use the platform dependent (UNIX like systems) parallelized version only for bigger input sizes (n > 20).

## greedy_knapsack returns better results
Our implementation of the greedy heuristic knapsack algorithm returns a better result than the ones in the test for larger n (we don't have this problem for smaller input sizes). In particular, we only fail the last two tests. Our outputs still respects the constraint of <= W. 

This is a description of the algorithm we used:

Step1: sort items in terms of the ratio value/weight in decreasing order
Step2: select the elements (given the constraint) with the biggest value/weight ratio until they fit in the knapsack
Step3: now do the same thing as above but instead of picking the elements with the biggest value/weight ratio we pick the elements with the biggest value until they fit in the knapsack
Step4: compare the solutions from step2 and step3 and pick the best

As you can see, this algorithm satisfies the greedy definition and has a time complexity of O(n) as the sort() function in R has a time complexity of O(n). 

We also noticed, that the only difference between the output in our implementation and the output of the last two tests is that our version returns some extra items (all the others are the same). It is because of these extra items that we get a better results. 

Probably, this is due to the fact that we can have objects with the same ratio v/w due to rounding. In this case, certain implementations can take an item as opposed to another with the same ratio only because that item shows up first. This can lead to performance differences. 
